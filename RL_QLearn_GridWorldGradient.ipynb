{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define world size\n",
    "grid_size = (100, 100)\n",
    "\n",
    "# Define Agent\n",
    "class Agent:\n",
    "    def __init__(self, position):\n",
    "        self.position = position\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "    def choose_action(self):\n",
    "        # Exploratory behaviour: Returns a random action\n",
    "        return np.random.choice(self.actions)\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        # Change the agent's position based on the action taken\n",
    "        x, y = self.position\n",
    "        if action == 'up' and x > 0:\n",
    "            self.position = (x - 1, y)\n",
    "        elif action == 'down' and x < grid_size[0] - 1:\n",
    "            self.position = (x + 1, y)\n",
    "        elif action == 'left' and y > 0:\n",
    "            self.position = (x, y - 1)\n",
    "        elif action == 'right' and y < grid_size[1] - 1:\n",
    "            self.position = (x, y + 1)\n",
    "\n",
    "# Initialize the agent\n",
    "start_position = (np.random.randint(0, grid_size[0]), np.random.randint(0, grid_size[1]))  # Random start position for the agent\n",
    "agent = Agent(start_position)\n",
    "agent.position\n",
    "\n",
    "# Define reward function\n",
    "def calculate_reward(agent_position, nutrient_position, previous_distance, found_nutrient_reward=1):\n",
    "    # Calculate the Manhattan distance from the agent to the nutrient source\n",
    "    current_distance = abs(agent_position[0] - nutrient_position[0]) + abs(agent_position[1] - nutrient_position[1])\n",
    "    \n",
    "    # Check if the agent has found the nutrient source\n",
    "    if current_distance == 0:\n",
    "        return found_nutrient_reward\n",
    "    \n",
    "    # Reward function --> PLAY WITH IT\n",
    "    if current_distance <= previous_distance:\n",
    "        reward_tmp = (100 / current_distance)\n",
    "        return reward_tmp # Moved closer\n",
    "    else:\n",
    "        reward_tmp = -(200 / current_distance) - 5\n",
    "        return reward_tmp # Moved away or stayed the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "class GridWorld:\n",
    "    def __init__(self, size, nutrient_position, nutrient_gradient):\n",
    "        self.size = size  # Grid size\n",
    "        self.nutrient_position = nutrient_position\n",
    "        self.nutrient_gradient = nutrient_gradient\n",
    "\n",
    "        # Initialize the agent within the GridWorld\n",
    "        start_position = (np.random.randint(0, self.size[0]), np.random.randint(0, self.size[1]))\n",
    "        self.agent = Agent(start_position)  # 'Agent' class needs to be defined beforehand\n",
    "\n",
    "        self.grid = np.zeros(size)  # Initialize a grid of zeros\n",
    "        self._set_nutrient()\n",
    "\n",
    "    def _set_nutrient(self, gradient_type='diamond'):  # Choose 'radial', 'rectangular', or 'diamond'\n",
    "        # Set the nutrient source on the grid\n",
    "        x_center, y_center = self.nutrient_position\n",
    "        self.grid[x_center, y_center] = 1  # Mark the nutrient source\n",
    "\n",
    "        if gradient_type == 'rectangular':\n",
    "            for i in range(1, self.nutrient_gradient):\n",
    "                gradient_area = [(x_center+dx, y_center+dy) for dx in range(-i, i+1) for dy in range(-i, i+1) if 0 <= x_center+dx < self.size[0] and 0 <= y_center+dy < self.size[1]]\n",
    "                for pos in gradient_area:\n",
    "                    self.grid[pos] += 0.1  # Increase the value to represent the gradient\n",
    "\n",
    "        elif gradient_type == 'radial':\n",
    "            for dx in range(-self.nutrient_gradient, self.nutrient_gradient):\n",
    "                for dy in range(-self.nutrient_gradient, self.nutrient_gradient):\n",
    "                    dist = np.sqrt(dx**2 + dy**2)\n",
    "                    if dist < self.nutrient_gradient and 0 <= x_center+dx < self.size[0] and 0 <= y_center+dy < self.size[1]:\n",
    "                        self.grid[x_center+dx, y_center+dy] += (self.nutrient_gradient - dist) / self.nutrient_gradient\n",
    "\n",
    "        elif gradient_type == 'diamond':\n",
    "            for dx in range(-self.nutrient_gradient, self.nutrient_gradient):\n",
    "                for dy in range(-self.nutrient_gradient, self.nutrient_gradient):\n",
    "                    dist = abs(dx) + abs(dy)  # Manhattan distance for a diamond shape\n",
    "                    if dist < self.nutrient_gradient and 0 <= x_center+dx < self.size[0] and 0 <= y_center+dy < self.size[1]:\n",
    "                        self.grid[x_center+dx, y_center+dy] += (self.nutrient_gradient - dist) / self.nutrient_gradient\n",
    "\n",
    "    def show_grid(self, agent_position=None):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(self.grid, cmap='hot', interpolation='nearest')\n",
    "\n",
    "        # If an agent's position is provided, overlay a green dot at the agent's position\n",
    "        if agent_position:\n",
    "            plt.scatter(agent_position[1], agent_position[0], color='green', s=25)  # x and y are swapped for scatter\n",
    "\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "    def reset(self):\n",
    "        # You can choose to reset the agent to a random position\n",
    "        # or to a fixed position like (0,0) every time\n",
    "        agent_position = (np.random.randint(0, self.size[0]), np.random.randint(0, self.size[1]))\n",
    "        \n",
    "        # If there are other variables that should be reset at the start of each episode,\n",
    "        # they should be reset here as well.\n",
    "        \n",
    "        # Return the initial state of the agent\n",
    "        return agent_position\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Assume that the agent has a method `take_action` which updates its position\n",
    "        previous_position = agent.position  # Store the previous position\n",
    "        agent.take_action(action)\n",
    "        new_position = agent.position\n",
    "        print(f\"Prev. Pos.: {previous_position}, New Pos.: {new_position}\")\n",
    "              \n",
    "        # Calculate the new distance to the nutrient source for the reward function\n",
    "        new_distance = abs(new_position[0] - self.nutrient_position[0]) + abs(new_position[1] - self.nutrient_position[1])\n",
    "        previous_distance = abs(previous_position[0] - self.nutrient_position[0]) + abs(previous_position[1] - self.nutrient_position[1])\n",
    "\n",
    "        # Call your reward function here\n",
    "        reward = calculate_reward(new_position, self.nutrient_position, previous_distance)\n",
    "\n",
    "        # Check if the agent has found the nutrient source\n",
    "        done = new_distance == 0\n",
    "\n",
    "        return new_position, reward, done\n",
    "\n",
    "# Parameters\n",
    "grid_size = (100, 100)\n",
    "nutrient_source_position = (50, 50)  # Center of the grid\n",
    "nutrient_gradient = 100\n",
    "\n",
    "# Initialize the environment\n",
    "environment = GridWorld(grid_size, nutrient_source_position, nutrient_gradient)\n",
    "environment._set_nutrient(gradient_type='radial') \n",
    "\n",
    "# Show the initial grid world\n",
    "environment.show_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate and visualize the agent's movement for 100 steps without prior training\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "for _ in range(100):\n",
    "    action = agent.choose_action()\n",
    "    agent.take_action(action)\n",
    "    clear_output(wait=True)  # Clear the previous output\n",
    "    environment.show_grid(agent.position)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING \n",
    "\n",
    "# Parameters\n",
    "grid_size = (100, 100)\n",
    "nutrient_source_position = (50, 50)  # Center of the grid\n",
    "nutrient_gradient = 100\n",
    "\n",
    "# Initialize the environment\n",
    "environment = GridWorld(grid_size, nutrient_source_position, nutrient_gradient)\n",
    "\n",
    "# Initialize the agent\n",
    "start_position = (np.random.randint(0, grid_size[0]), np.random.randint(0, grid_size[1]))  # Random start position for the agent\n",
    "agent = Agent(start_position)\n",
    "\n",
    "# Define the number of episodes and the maximum number of steps per episode --> PLAY WITH THEM\n",
    "num_episodes = 100\n",
    "max_steps_per_episode = 10000000\n",
    "\n",
    "# Learning parameters --> PLAY WITH THEM\n",
    "alpha = 0.4 # Learning rate\n",
    "gamma = 0.99 # Discount factor\n",
    "epsilon = 1.0  # Starting value for epsilon-greedy strategy\n",
    "min_epsilon = 0.005\n",
    "epsilon_decay = 0.5\n",
    "\n",
    "# Keep track of the learning progress\n",
    "all_epochs = []  # Stores the number of moves it takes to reach the goal for each episode\n",
    "all_penalties = []  # Stores the number of penalties the agent incurs in each episode\n",
    "all_states = []  # Stores the spawning position of the agent in each episode\n",
    "\n",
    "# Define the mappings outside of the loop, so they are only defined once\n",
    "action_to_index = {'up': 0, 'down': 1, 'left': 2, 'right': 3}\n",
    "index_to_action = {0: 'up', 1: 'down', 2: 'left', 3: 'right'}\n",
    "\n",
    "# Initialize Q-table\n",
    "q_table = np.zeros((*grid_size, len(agent.actions)))\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = (np.random.randint(0, grid_size[0]), np.random.randint(0, grid_size[1]))  # Random start position for the agent\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    all_states.append(state)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = agent.choose_action()  # This returns a random string e.g. 'up', 'down', etc.\n",
    "        else:\n",
    "            # Exploit learned values using the current state\n",
    "            action_index = np.argmax(q_table[state])\n",
    "            action = index_to_action[action_index]  # Convert index back to the action string if necessary\n",
    "\n",
    "        # Take the action and get the new state and reward from the environment\n",
    "        new_state, reward, done = environment.step(action)\n",
    "\n",
    "        # Convert the action string to an index using the mapping\n",
    "        action_index = action_to_index[action]  # Now action_index is an integer\n",
    "\n",
    "        # Now use the integer index for the Q-table\n",
    "        old_value = q_table[state + (action_index,)]\n",
    "        next_max = np.max(q_table[new_state])\n",
    "\n",
    "        # Update the Q-table using the integer action index\n",
    "        q_table[state + (action_index,)] = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "\n",
    "        if reward < 0:  # Assuming that negative rewards are penalties\n",
    "            penalties += 1\n",
    "        \n",
    "        state = new_state\n",
    "        epochs += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decaying epsilon value to reduce the number of exploratory moves as it learns\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    \n",
    "    all_epochs.append(epochs)\n",
    "    all_penalties.append(penalties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate and visualize the agent's movement for 100 steps after training; results will be saved as GIF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import imageio\n",
    "\n",
    "# Setup the environment and agent to the initial state for the demonstration\n",
    "start_position = (np.random.randint(0, grid_size[0]), np.random.randint(0, grid_size[1]))\n",
    "agent = Agent(start_position)\n",
    "state = agent.position\n",
    "epsilon = 0 # Suppresses \n",
    "\n",
    "# Prepare the plot\n",
    "plt.ion()  # Turn on interactive mode\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))  # Two subplots: one for the grid and one for the reward\n",
    "\n",
    "rewards = []  # List to keep track of the rewards\n",
    "\n",
    "# Prepare the plot\n",
    "plt.ioff()  # Turn off interactive mode for GIF creation\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))  # Two subplots: one for the grid and one for the reward\n",
    "\n",
    "frames = []  # List to keep track of the frames for the GIF\n",
    "\n",
    "# Simulate and visualize the agent's movement for 100 steps\n",
    "for t in range(100):\n",
    "    # Choose the best action from the Q-table for the current state\n",
    "    action_index = np.argmax(q_table[state])\n",
    "    action = index_to_action[action_index]\n",
    "\n",
    "    # Perform the chosen action\n",
    "    new_state, reward, done = environment.step(action)\n",
    "\n",
    "    # Update rewards list\n",
    "    rewards.append(reward)\n",
    "\n",
    "    # Update the state and the agent's position\n",
    "    state = new_state\n",
    "    agent.position = state\n",
    "\n",
    "    # Clear current output and display the grid and reward plot\n",
    "    clear_output(wait=True)\n",
    "    display(fig)  # Display the figure\n",
    "    \n",
    "    # Plot the grid\n",
    "    ax[0].clear()\n",
    "    ax[0].imshow(environment.grid, cmap='hot', interpolation='nearest')\n",
    "    ax[0].scatter(agent.position[1], agent.position[0], color='green', s=25)  # Plot the agent\n",
    "    ax[0].set_title('Agent Movement')\n",
    "\n",
    "    # Plot the rewards\n",
    "    ax[1].clear()\n",
    "    ax[1].plot(rewards, label='Reward')\n",
    "    ax[1].set_title('Reward per Step')\n",
    "    ax[1].set_xlabel('Step')\n",
    "    ax[1].set_ylabel('Reward')\n",
    "    ax[1].legend()\n",
    "\n",
    "    # Save the current frame\n",
    "    fig.canvas.draw()\n",
    "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    frames.append(image)\n",
    "\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t + 1))\n",
    "        break\n",
    "\n",
    "# Save frames as a GIF\n",
    "imageio.mimsave('agent_movement.gif', frames, fps=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
